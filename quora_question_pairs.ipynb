{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "import csv\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_symbols = {'?','.'}\n",
    "pos_idx = dict(ADJ=0, ADP=1, ADV=2, CONJ=3, DET=4, NOUN=5, NUM=6, PRT=7, PRON=8, VERB=9, X=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec matrix\n",
    "w2v = Word2Vec.load(\"emb_tr\")\n",
    "# w2v = KeyedVectors.load_word2vec_format(\"../../../../../../../GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "vec_size = w2v.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing\n",
    "def extend_abbre(question):\n",
    "    question = question.lower().replace(\"won't\",\"will not\").replace(\"n't\",\"not\").replace(\"it's\",\"it is\") \\\n",
    "    .replace(\"'ve\",\" have\").replace(\"i'm\",\"i am\").replace(\"'re\",\" are\").replace(\"he's\",\"he is\").replace(\"she's\",\"she is\") \\\n",
    "    .replace(\"'s\",\" own\").replace(\"what's\",\"what is\").replace(\"+\",\"plus\").replace(\"'ll\",\" will\").replace(\"'d\",\" would\") \\\n",
    "    .replace(\"#\",\"sharp\").replace(\"=\",\" equal\").replace(\",000\",\"000\").replace(\"&\",\"and\").replace(\"|\",\"or\")\n",
    "    return question\n",
    "\n",
    "def stemmer(train_data):\n",
    "    stem_tool = SnowballStemmer('english')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    train_data['question1'] = train_data.question1.map(lambda x: ' '.join([stem_tool.stem(word) for word in word_tokenize(extend_abbre(str(x))) if not word in stop_words]))\n",
    "    train_data['question2'] = train_data.question2.map(lambda x: ' '.join([stem_tool.stem(word) for word in word_tokenize(extend_abbre(str(x))) if not word in stop_words]))\n",
    "    #now it is bytes in the train_data\n",
    "    #if want to convert to string using encode('UTF-8')\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dif_length(s1, s2):\n",
    "    return abs(len(s1) - len(s2))\n",
    "\n",
    "def dif_word_count(s1, s2):\n",
    "    return abs(len(s1.split()) - len(s2.split()))\n",
    "\n",
    "def is_ending_symbol_identical(s1, s2):\n",
    "    if s1 and s2 and s1[-1] in end_symbols and s2[-1] in end_symbols:\n",
    "        return int(s1[-1] == s2[-1])\n",
    "    return 1\n",
    "\n",
    "def is_first_word_identical(s1, s2):\n",
    "    if s1 and s2:\n",
    "        return int(s1.split()[0] ==  s2.split()[0] )\n",
    "    return 0\n",
    "\n",
    "def count_shared_words(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "    ret = 0\n",
    "    for word in s1.split():\n",
    "        if word in s2:\n",
    "            ret += 1\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sentence similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parts_of_speech(s):\n",
    "        if type(s) == str:                                                 \n",
    "                tokens = word_tokenize(s)\n",
    "        else:\n",
    "                tokens = s \n",
    "#         n = len(tokens)                                                 \n",
    "        tokens_and_tags = pos_tag(tokens, tagset = \"universal\")\n",
    "#         tags = [t for (_,t) in tokens_and_tags]\n",
    "        return(tokens_and_tags)\n",
    "\n",
    "pos_idx = dict(ADJ=0, ADP=1, ADV=2, CONJ=3, DET=4, NOUN=5, NUM=6, PRT=7, PRON=8, VERB=9, X=10)\n",
    "\n",
    "def sent_embedding(pos_li, vec_size):\n",
    "    pos_groups = [ np.zeros((vec_size)) for i in range(len(pos_idx)) ]\n",
    "    for word, pos in pos_li:\n",
    "        if pos != '.':\n",
    "            if word in w2v.wv:\n",
    "                pos_groups[pos_idx[pos]] += w2v.wv[word]\n",
    "    return pos_groups\n",
    "\n",
    "def sent_cos_similarity(pos_group1, pos_group2):\n",
    "    ret = []\n",
    "    for i, _ in enumerate(pos_group1):\n",
    "#         ret += np.exp(-(np.abs(pos_group1[i] - pos_group2[i])))\n",
    "#           ret.append(np.dot(pos_group1[i], pos_group2[i])/(np.linalg.norm(pos_group1[i])* np.linalg.norm(pos_group2[i])))\n",
    "        ret.append((cosine_similarity(pos_group1[i].reshape(1, -1), pos_group2[i].reshape(1, -1)))[0][0])\n",
    "    return ret\n",
    "                \n",
    "# return a vector represents the similarity of two sentence\n",
    "def cos_similarity_vector(s1, s2, w2v_vec_size):\n",
    "    pos_li_1 = parts_of_speech(s1)\n",
    "    pos_li_2 = parts_of_speech(s2)\n",
    "    \n",
    "    # assign the words in a sentence into each of the 11 POS groups\n",
    "    # substitute each word in each POS group with the word's embedding vector\n",
    "    # sum the vectors in each POS group of a sentence\n",
    "    # so we will have a list of 11 vectors for each sentence\n",
    "    \n",
    "    emb_list_1 = sent_embedding(pos_li_1, w2v_vec_size)\n",
    "    emb_list_2 = sent_embedding(pos_li_2, w2v_vec_size)\n",
    "    # compute the cosine_similarity of the corresponding groups in the two sentences\n",
    "    # so we will have a list of 11 cosine_similarity measures.\n",
    "    \n",
    "    return sent_cos_similarity(emb_list_1, emb_list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### category_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_news_dataset():\n",
    "    twenty_train = fetch_20newsgroups(subset='train', \\\n",
    "                                      remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=22) \n",
    "    twenty_test = fetch_20newsgroups(subset='test', \\\n",
    "                                     remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=22)\n",
    "    return twenty_train, twenty_test\n",
    "\n",
    "def extract_text_features(train_data, min_docs):\n",
    "    \n",
    "    sw = set(stopwords.words('english'))\n",
    "    \n",
    "    cv = CountVectorizer(stop_words = sw, min_df = min_docs)\n",
    "    X_train_counts = cv.fit_transform(train_data)\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    \n",
    "    return X_train_tfidf, cv, tfidf_transformer\n",
    "\n",
    "def extract_text_features_from_test_data(test_data, min_docs, cv, tfidf_transformer):\n",
    "    X_test_counts = cv.transform(test_data)\n",
    "    X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "    \n",
    "    return X_test_tfidf\n",
    "    \n",
    "def train_multinomialNB(X_train, Y_train):\n",
    "    clf = MultinomialNB().fit(X_train, Y_train) \n",
    "    return clf\n",
    "\n",
    "def train_LR(X_train, Y_train):\n",
    "    learner = lr()\n",
    "    learner.fit(X_train, Y_train)\n",
    "    return learner\n",
    "\n",
    "twenty_train, twenty_test = load_news_dataset()\n",
    "X_train_tfidf, cv, tfidf_transformer = extract_text_features(twenty_train.data, 1)\n",
    "category_lr = train_LR(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "cleaned = stemmer(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create matrix of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_limit = 404000\n",
    "doc_limit = 10000\n",
    "feature_count = 3+len(pos_idx)+1+1;\n",
    "Xtr = np.zeros((doc_limit, feature_count))\n",
    "Ytr = []\n",
    "\n",
    "\n",
    "# with open('data/train.csv','r',  encoding='utf-8') as tr_data:\n",
    "#     tr_csv = csv.reader(tr_data)\n",
    "#     next(tr_csv)\n",
    "for i, row in enumerate(cleaned[\"question1\"]):\n",
    "    q_left = row\n",
    "    q_right = cleaned[\"question2\"][i];\n",
    "\n",
    "    if type(q_left) == str and type(q_right) == str:\n",
    "        cos_sim = cos_similarity_vector(q_left, q_right, vec_size)\n",
    "        nz = [i for i in cos_sim if i > 0]\n",
    "        if len(nz) == 0:\n",
    "            cos_sim_avg = 0\n",
    "        else:\n",
    "            cos_sim_avg = sum(cos_sim)/(len(nz))\n",
    "\n",
    "        left_cat = category_lr.predict(extract_text_features_from_test_data([q_left], 1, cv, tfidf_transformer))\n",
    "        right_cat = category_lr.predict(extract_text_features_from_test_data([q_right], 1, cv, tfidf_transformer))\n",
    "\n",
    "        Xtr[i,:] = np.array([is_ending_symbol_identical(q_left,q_right), is_first_word_identical(q_left,q_right),\\\n",
    "                              count_shared_words(q_left,q_right), cos_sim_avg, \\\n",
    "                             int(left_cat == right_cat)]+cos_sim)\n",
    "#         if type(q_left) == str and type(q_right) == str:\n",
    "#             Xtr[i,:] = np.array([dif_length(q_left,q_right), dif_word_count(q_left,q_right),\\\n",
    "#                                   is_ending_symbol_identical(q_left,q_right), is_first_word_identical(q_left,q_right),\\\n",
    "#                                   count_shared_words(q_left,q_right)])\n",
    "        Ytr.append(row[-1]) \n",
    "\n",
    "    if i >= doc_limit-1:\n",
    "        break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and predict with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_percentage = .8\n",
    "sep = int(doc_limit*training_percentage)\n",
    "\n",
    "xtr, ytr, xte, yte = Xtr[:sep,:],Ytr[:sep], Xtr[sep:,:], Ytr[sep:]\n",
    "\n",
    "learner = lr()\n",
    "learner.fit(xtr, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_LR = learner.predict(xte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1974 / 2000 = 0.987\n"
     ]
    }
   ],
   "source": [
    "# a matching pair means a correct prediction\n",
    "#print('result',list(zip(predicted_LR,yte)))\n",
    "print('accuracy:', sum([int(l==r) for l,r in zip(predicted_LR,yte)]), \"/\" ,len(predicted_LR), \"=\", \\\n",
    "      sum([int(l==r) for l,r in zip(predicted_LR,yte)])/len(predicted_LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanz\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9901579725542419\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(learner, Xtr, Ytr, cv = 10)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/test.csv')\n",
    "cleaned_test = stemmer(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_1 = cleaned_test[\"question1\"]\n",
    "col_2 = cleaned_test[\"question2\"]\n",
    "doc_count = len(col_1)\n",
    "\n",
    "Xte = np.zeros((doc_count, feature_count))\n",
    "\n",
    "for i, _ in enumerate(col_1):\n",
    "    q_left = col_1[i]\n",
    "    q_right = col_2[i]\n",
    "    \n",
    "    if type(q_left) == str and type(q_right) == str:\n",
    "            cos_sim = cos_similarity_vector(q_left, q_right, vec_size)\n",
    "            nz = [i for i in cos_sim if i > 0]\n",
    "            if len(nz) == 0:\n",
    "                cos_sim_avg = 0\n",
    "            else:\n",
    "                cos_sim_avg = sum(cos_sim)/(len(nz))\n",
    "\n",
    "            left_cat = category_lr.predict(extract_text_features_from_test_data([q_left], 1, cv, tfidf_transformer))\n",
    "            right_cat = category_lr.predict(extract_text_features_from_test_data([q_right], 1, cv, tfidf_transformer))\n",
    "\n",
    "            Xte[i,:] = np.array([is_ending_symbol_identical(q_left,q_right), is_first_word_identical(q_left,q_right),\\\n",
    "                                  count_shared_words(q_left,q_right), cos_sim_avg, \\\n",
    "                                 int(left_cat == right_cat)]+cos_sim)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_kaggle = learner.predict(Xte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing complete\n"
     ]
    }
   ],
   "source": [
    "myData = [\"test_id\", \"is_duplicate\"]\n",
    " \n",
    "myFile = open('kaggle_submit1.csv', 'w')\n",
    "with myFile:\n",
    "    writer = csv.writer(myFile)\n",
    "    writer.writerow(myData)\n",
    "    for i, result in enumerate(predicted_kaggle):\n",
    "        writer.writerow([i, result])\n",
    "print(\"Writing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
