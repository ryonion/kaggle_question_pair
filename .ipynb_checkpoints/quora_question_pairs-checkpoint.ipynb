{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "end_symbols = {'?','.'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dif_length(s1, s2):\n",
    "    return abs(len(s1) - len(s2))\n",
    "\n",
    "def dif_word_count(s1, s2):\n",
    "    return abs(len(s1.split()) - len(s2.split()))\n",
    "\n",
    "def is_ending_symbol_identical(s1, s2):\n",
    "    if s1 and s2 and s1[-1] in end_symbols and s2[-1] in end_symbols:\n",
    "        return int(s1[-1] == s2[-1])\n",
    "    return 1\n",
    "\n",
    "def is_first_word_identical(s1, s2):\n",
    "    if s1 and s2:\n",
    "        return int(s1.split()[0] ==  s2.split()[0] )\n",
    "    return 0\n",
    "\n",
    "def count_shared_words(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "    ret = 0\n",
    "    for word in s1.split():\n",
    "        if word in s2:\n",
    "            ret += 1\n",
    "    return ret\n",
    "\n",
    "def diff_pov(s1, s2):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### learner class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class qqp_learner:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    #..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create matrix of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_limit = 5000\n",
    "feature_count = 5;\n",
    "Xtr = np.zeros((doc_limit, feature_count))\n",
    "Ytr = []\n",
    "with open('data/train.csv','r',  encoding='utf-8') as tr_data:\n",
    "    tr_csv = csv.reader(tr_data)\n",
    "    for i, row in enumerate(tr_csv):\n",
    "        q_left = row[3];\n",
    "        q_right = row[4];\n",
    "        Xtr[i,:] = np.array([dif_length(q_left,q_right), dif_word_count(q_left,q_right),\\\n",
    "                              is_ending_symbol_identical(q_left,q_right), is_first_word_identical(q_left,q_right),\\\n",
    "                              count_shared_words(q_left,q_right)])\n",
    "        Ytr.append(row[-1]) \n",
    "        if i >= doc_limit-1:\n",
    "            break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and predict with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_percentage = .6\n",
    "sep = int(doc_limit*training_percentage)\n",
    "\n",
    "xtr, ytr, xte, yte = Xtr[:sep,:],Ytr[:sep], Xtr[sep:,:], Ytr[sep:]\n",
    "\n",
    "learner = lr()\n",
    "learner.fit(xtr, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_LR = learner.predict(xte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1276 / 2000 = 0.638\n"
     ]
    }
   ],
   "source": [
    "# a matching pair means a correct prediction\n",
    "#print('result',list(zip(predicted_LR,yte)))\n",
    "print('accuracy:', sum([int(l==r) for l,r in zip(predicted_LR,yte)]), \"/\" ,len(predicted_LR), \"=\", \\\n",
    "      sum([int(l==r) for l,r in zip(predicted_LR,yte)])/len(predicted_LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
